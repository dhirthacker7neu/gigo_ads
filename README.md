# gigo_ads

<br>**Query:** openai embeddings<br><br>**Nomic Embed: Training a Reproducible Long Context Text Embedder**<br>**Summary:**
The technical report details the training of nomic-embed-text-v1, an English text embedding model that surpasses OpenAI's Ada-002 and text-embedding-3-small models on short and long-context tasks. It is the first fully reproducible, open-source, open-weights, open-data model with an 8192 context length. The training code and model weights are released under an Apache 2 license, along with a data loader containing 235 million curated text pairs for full replication. Access the code and data at https://github.com/nomic-ai/contrastors.<br><a href="http://arxiv.org/pdf/2402.01613v1" style="color: blue;">[Download PDF]</a>

<br>**Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for   Semantic Representations**<br>**Summary:**
This work explores finetuning Malaysian language models, particularly Llama2 and Mistral, for embedding tasks involving negative and positive pairs. Two models are released for Semantic Similarity and Retrieval-Augmented Generation tasks, with the Llama2 model outperforming OpenAI's text-embedding-ada-002 across various metrics. For RAG models, the Llama2 model competes well with OpenAI's model and excels in keyword research papers and lom.agc.gov.my datasets. The study demonstrates the effectiveness of the finetuning strategy and the improved performance in both tasks.<br><a href="http://arxiv.org/pdf/2402.03053v1" style="color: blue;">[Download PDF]</a>

<br>**A Comparative Study of Transformers on Word Sense Disambiguation**<br>**Summary:**
Recent research in Natural Language Processing has focused on training large models to generate context-aware language representations, resulting in Contextualized Word Embeddings (CWEs). This paper presents a comparative study of nine popular Transformer models, evaluating their contextualization power using Word Sense Disambiguation tasks. The study uses a k-Nearest Neighbor classification on CWEs and shows superior results compared to current state-of-the-art methods on SensEval-2 and SensEval-3 tasks.<br><a href="http://arxiv.org/pdf/2111.15417v1" style="color: blue;">[Download PDF]</a>

<br>**Vector Search with OpenAI Embeddings: Lucene Is All You Need**<br>**Summary:**
The text demonstrates how vector search can be implemented using Lucene on the MS MARCO passage ranking test collection without the need for a dedicated vector store. The use of hierarchical navigable small-world network (HNSW) indexes in Lucene is shown to be sufficient for vector search capabilities in a standard bi-encoder architecture. This challenges the idea that a dedicated vector store is necessary for utilizing deep neural networks in search, as existing infrastructure can support it effectively.<br><a href="http://arxiv.org/pdf/2308.14963v1" style="color: blue;">[Download PDF]</a>

<br>**Vec2Vec: A Compact Neural Network Approach for Transforming Text   Embeddings with High Fidelity**<br>**Summary:**
Vector embeddings are widely used in language tasks, with OpenAI's text-ada-002 being a popular model with 1,536-dimensional vectors for 6,000 words. To make this model more accessible, a neural network was trained to convert open-source 768-dimensional MPNet embeddings into text-ada-002 embeddings. This model achieved a high cosine similarity on unseen reviews and was able to retrieve relevant results, offering a lightweight and fast alternative to the proprietary model. Improvements are planned for training with a larger dataset for better performance and interoperability benefits.<br><a href="http://arxiv.org/pdf/2306.12689v1" style="color: blue;">[Download PDF]</a>

